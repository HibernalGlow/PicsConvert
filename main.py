import os
import sys
import threading
import argparse
from pathlib import Path
from typing import Dict, Any, List, Tuple, Set
import time
import json # æ–°å¢å¯¼å…¥
from functools import partial # æ–°å¢å¯¼å…¥

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
from src.utils.input_handler import InputHandler
from src.convert.format_convert import ArchiveConverter,SUPPORTED_ARCHIVE_FORMATS
from src.convert.performance_control import get_performance_params,start_config_gui_thread
# å¯¼å…¥é»‘åå•æ–‡ä»¶è·¯å¾„
from src.convert.compression_tracker import BLACKLIST_FILE_PATH
from src.utils.monitor_decorator import infinite_monitor
from textual_preset import create_config_app
from textual_logger import TextualLoggerManager
import zipfile
from concurrent.futures import ThreadPoolExecutor

# è·å–loggerå®ä¾‹
from loguru import logger
import os
import sys
from pathlib import Path
from datetime import datetime

def setup_logger(app_name="app", project_root=None, console_output=True):
    """é…ç½® Loguru æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        app_name: åº”ç”¨åç§°ï¼Œç”¨äºæ—¥å¿—ç›®å½•
        project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        console_output: æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns:
        tuple: (logger, config_info)
            - logger: é…ç½®å¥½çš„ logger å®ä¾‹
            - config_info: åŒ…å«æ—¥å¿—é…ç½®ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    if project_root is None:
        project_root = Path(__file__).parent.resolve()
    
    # æ¸…é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æœ‰æ¡ä»¶åœ°æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆç®€æ´ç‰ˆæ ¼å¼ï¼‰
    if console_output:
        logger.add(
            sys.stdout,
            level="INFO",
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <blue>{elapsed}</blue> | <level>{level.icon} {level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>"
        )
    
    # ä½¿ç”¨ datetime æ„å»ºæ—¥å¿—è·¯å¾„
    current_time = datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    hour_str = current_time.strftime("%H")
    minute_str = current_time.strftime("%M%S")
    
    # æ„å»ºæ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    log_dir = os.path.join(project_root, "logs", app_name, date_str, hour_str)
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{minute_str}.log")
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        log_file,
        level="DEBUG",
        rotation="10 MB",
        retention="30 days",
        compression="zip",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {elapsed} | {level.icon} {level: <8} | {name}:{function}:{line} - {message}",
    )
    
    # åˆ›å»ºé…ç½®ä¿¡æ¯å­—å…¸
    config_info = {
        'log_file': log_file,
    }
    
    logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œåº”ç”¨åç§°: {app_name}")
    return logger, config_info


logger, config_info = setup_logger(app_name="pics_convert", console_output=False)

USE_RICH = False  # æ˜¯å¦ä½¿ç”¨Richåº“è¿›è¡Œè¾“å‡º

# --- åŠ è½½é…ç½®æ–‡ä»¶ ---
CONFIG_FILE_PATH = Path(current_dir) / 'config.json'
APP_CONFIG = {}
LAYOUT_CONFIG = {} # æä¾›é»˜è®¤ç©ºé…ç½®ä»¥é¿å…åç»­é”™è¯¯
PRESET_CONFIGS = {}

try:
    with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:
        APP_CONFIG = json.load(f)
    LAYOUT_CONFIG = APP_CONFIG.get("layout", {})
    PRESET_CONFIGS = APP_CONFIG.get("presets", {})
    logger.info(f"[#file]æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {CONFIG_FILE_PATH}")
except FileNotFoundError:
    logger.error(f"[#file]é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {CONFIG_FILE_PATH}")
except json.JSONDecodeError:
    logger.error(f"[#file]è§£æé…ç½®æ–‡ä»¶å¤±è´¥: {CONFIG_FILE_PATH}")
except Exception as e:
    logger.error(f"[#file]åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
# --- ç»“æŸåŠ è½½é…ç½®æ–‡ä»¶ ---


# å®šä¹‰éœ€è¦è·³è¿‡çš„æ ¼å¼åˆ—è¡¨
SKIP_FORMATS: Set[str] = {
    '.avif', '.jxl', '.webp'  # é»˜è®¤è·³è¿‡è¿™äº›æ ¼å¼
}

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨è·³è¿‡æ ¼å¼åˆ—è¡¨ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–
ACTIVE_SKIP_FORMATS: Set[str] = SKIP_FORMATS.copy()

# å®šä¹‰éœ€è¦è·³è¿‡çš„æ–‡ä»¶åå…³é”®è¯
SKIP_KEYWORDS: Set[str] = {
    '_avif', '_jxl', '_webp',  # é»˜è®¤è·³è¿‡åŒ…å«è¿™äº›å…³é”®è¯çš„æ–‡ä»¶
    'avif_', 'jxl_', 'webp_',
    '.avif', '.jxl', '.webp'
}

# å®šä¹‰è·¯å¾„é»‘åå•å…³é”®è¯åˆ—è¡¨
BLACKLIST_PATHS: Set[str] = {
    'temp_'  # é»˜è®¤è·³è¿‡åŒ…å«è¿™äº›å…³é”®è¯çš„è·¯
}

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æ´»è·ƒçš„é»‘åå•è·¯å¾„å…³é”®è¯ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–
ACTIVE_BLACKLIST_PATHS: Set[str] = BLACKLIST_PATHS.copy()

def init_layout():
    # ä½¿ç”¨ä» JSON åŠ è½½çš„ LAYOUT_CONFIG
    if LAYOUT_CONFIG:
        TextualLoggerManager.set_layout(LAYOUT_CONFIG, config_info['log_file'])
    else:
        logger.warning("[#file]å¸ƒå±€é…ç½®æœªåŠ è½½ï¼Œæ— æ³•åˆå§‹åŒ– Textual å¸ƒå±€")


def process_archive(*args, **kwargs) -> None:
    """å¤„ç†å•ä¸ªå‹ç¼©åŒ…
    
    Args:
        archive_path: å‹ç¼©åŒ…è·¯å¾„
        filter_params: è¿‡æ»¤å‚æ•°å­—å…¸
        **kwargs: å…¶ä»–å‚æ•°
            - min_width: æœ€å°å›¾ç‰‡å®½åº¦
            - thread_count: çº¿ç¨‹æ•° 
            - batch_size: æ‰¹å¤„ç†å¤§å°
            - infinite_mode: æ˜¯å¦æ— é™æ¨¡å¼
            - interval_minutes: ç›‘æ§é—´éš”(åˆ†é’Ÿ)
    """

    # æå–å¿…è¦å‚æ•°
    archive_path = args[0] if args else kwargs.get('archive_path')
    filter_params = kwargs.get('filter_params', {})
    
    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
    file_ext = Path(archive_path).suffix.lower()
    if file_ext not in SUPPORTED_ARCHIVE_FORMATS:
        logger.info(f"[#archive]ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        return
    
    # ç›‘æ§æ€§èƒ½å‚æ•°å¹¶å¤„ç†æš‚åœé€»è¾‘
    def check_performance_params(current_thread_count=None, current_batch_size=None):
        nonlocal thread_count, batch_size
        new_thread_count, new_batch_size, is_paused = get_performance_params()
        
        # æ£€æŸ¥å‚æ•°æ˜¯å¦å‘ç”Ÿå˜åŒ–
        if new_thread_count != current_thread_count or new_batch_size != current_batch_size:
            logger.info(f"[#performance]ğŸ§µ çº¿ç¨‹æ•°: {new_thread_count} | æ‰¹å¤„ç†: {new_batch_size}")
            thread_count, batch_size = new_thread_count, new_batch_size
            
        # å¤„ç†æš‚åœé€»è¾‘
        if is_paused:
            logger.info(f"[#performance]â¸ å¤„ç†å·²æš‚åœ: {archive_path}")
            while is_paused:
                time.sleep(0.5)  # é˜²æ­¢è¿‡äºé¢‘ç¹çš„æ£€æŸ¥
                _, _, is_paused = get_performance_params()
                # åœ¨æš‚åœçŠ¶æ€ä¸‹ç»§ç»­ç›‘æ§å‚æ•°å˜åŒ–
                check_performance_params(thread_count, batch_size)
                
            logger.info(f"[#performance]â–¶ å¤„ç†å·²æ¢å¤: {archive_path}")
        
        return thread_count, batch_size, is_paused
    
    # åˆå§‹åŒ–æ€§èƒ½å‚æ•°
    thread_count, batch_size, is_paused = get_performance_params()
    logger.info(f"[#performance]ğŸ§µ çº¿ç¨‹æ•°: {thread_count} | æ‰¹å¤„ç†: {batch_size}")
    
    # åˆæ¬¡æ£€æŸ¥æ˜¯å¦æš‚åœ
    thread_count, batch_size, is_paused = check_performance_params(thread_count, batch_size)
    
    # è®¾ç½®å®šæœŸæ£€æŸ¥è®¡æ—¶å™¨
    last_check_time = time.time()
    check_interval = 2.0  # ç§’

    
    # ä¿®æ”¹è½¬æ¢å™¨é…ç½®å‚æ•°
    converter_params = {
        'thread_count': thread_count,
        'min_width': filter_params.get('min_width', -1),
        'enable_jxl_fallback': kwargs.get('jxlfall', True),  # å¯ç”¨JXLå›é€€
        'target_format': kwargs.get('format', 'avif').lower(),    # ç¡®ä¿æ ¼å¼å°å†™
        'quality': int(kwargs.get('quality', 90)),                # ç¡®ä¿è´¨é‡æ˜¯æ•´æ•°
        'lossless': kwargs.get('lossless', False)                 # æ·»åŠ æ— æŸé€‰é¡¹
    }
    
    converter = ArchiveConverter(converter_params)
    try:
        converter.convert_archive(archive_path)
        logger.info(f"[#archive]âœ… æˆåŠŸå¤„ç†: {archive_path}")
    except Exception as e:
        logger.info(f"[#archive]âŒ å¤„ç†å¤±è´¥: {archive_path} - {str(e)}")


def process_archives(archive_paths: List[str], **kwargs) -> None:
    """æ‰¹é‡å¤„ç†å‹ç¼©åŒ…ï¼Œæ”¯æŒæ— é™æ¨¡å¼ç›‘æ§
    
    Args:
        archive_paths: å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨
        **kwargs: å…¶ä»–å‚æ•°
            - filter_params: è¿‡æ»¤å‚æ•°å­—å…¸
            - interval_minutes: ç›‘æ§é—´éš”(åˆ†é’Ÿ)
            - directories: ç›‘æ§çš„ç›®å½•åˆ—è¡¨
            - archive_path: (å¯é€‰) å…³è”çš„å‹ç¼©åŒ…è·¯å¾„ï¼Œç”¨äºé»‘åå•åŠŸèƒ½
    """
    # æ·»åŠ æ€»è¿›åº¦è®°å½•
    total_files = len(archive_paths)
    current_file = 0
    logger.info(f"[#status]å¼€å§‹å¤„ç†,å…±{total_files}ä¸ªæ–‡ä»¶")
    
    # æ ¹æ®æ¨¡å¼å¤„ç†æ–‡ä»¶
    for archive_path in archive_paths:
        current_file += 1
        progress = (current_file / total_files) * 100
        logger.info(f"[@status]æ€»è¿›åº¦:({current_file}/{total_files}) {progress:.1f}% ")
        logger.info(f"[#archive]å¤„ç†: {archive_path}")
        
        # è°ƒç”¨å•ä¸ªå‹ç¼©åŒ…å¤„ç†å‡½æ•°ï¼Œä¼ é€’ archive_path
        process_kwargs = kwargs.copy()
        process_kwargs['archive_path'] = archive_path # ç¡®ä¿ archive_path ä¼ é€’ä¸‹å»
        process_archive(**process_kwargs)
    
    # å¤„ç†å®Œæˆåè¾“å‡ºæœ€ç»ˆè¿›åº¦
    logger.info(f"[#status]å¤„ç†å®Œæˆ - å…±å¤„ç†{total_files}ä¸ªæ–‡ä»¶")

def check_archive_skip(archive_path: str, json_blacklist: Set[str]) -> Tuple[str, bool, str]: # æ–°å¢ json_blacklist å‚æ•°
    """æ£€æŸ¥å‹ç¼©åŒ…æ˜¯å¦åº”è¯¥è¢«è·³è¿‡
    
    Args:
        archive_path: å‹ç¼©åŒ…è·¯å¾„
        json_blacklist: ä»JSONæ–‡ä»¶åŠ è½½çš„é»‘åå•è·¯å¾„é›†åˆ
        
    Returns:
        Tuple[str, bool, str]: (å‹ç¼©åŒ…è·¯å¾„, æ˜¯å¦è·³è¿‡, è·³è¿‡åŸå› )
    """
    try:
        resolved_path_str = str(Path(archive_path).resolve())

        # 1. æ£€æŸ¥JSONé»‘åå•
        if resolved_path_str in json_blacklist:
            logger.info(f"[#archive]è·³è¿‡JSONé»‘åå•ä¸­çš„å‹ç¼©åŒ…: {resolved_path_str}")
            return (archive_path, True, "json_blacklist")

        # 2. æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«å‘½ä»¤è¡ŒæŒ‡å®šçš„é»‘åå•å…³é”®è¯
        if ACTIVE_BLACKLIST_PATHS:
            full_path_lower = resolved_path_str.lower()
            if any(keyword.lower() in full_path_lower for keyword in ACTIVE_BLACKLIST_PATHS):
                logger.info(f"[#archive]è·³è¿‡å‘½ä»¤è¡Œé»‘åå•è·¯å¾„çš„å‹ç¼©åŒ…: {resolved_path_str}")
                return (archive_path, True, "keyword_blacklist")
        
        # 3. æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«éœ€è¦è·³è¿‡çš„å…³é”®è¯ (å¦‚æœéœ€è¦å¯ä»¥å–æ¶ˆæ³¨é‡Š)
        # file_name = Path(archive_path).stem.lower()  # å–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨æ–‡ä»¶åæ£€æŸ¥
        # if any(keyword in file_name for keyword in SKIP_KEYWORDS):
        #     logger.info(f"[#archive]è·³è¿‡æ–‡ä»¶ååŒ…å«å…³é”®è¯çš„å‹ç¼©åŒ…: {archive_path}")
        #     return (archive_path, True, "name")
        
        # 4. å¦‚æœè·³è¿‡æ ¼å¼åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™ä¸è·³è¿‡ä»»ä½•æ–‡ä»¶
        if not ACTIVE_SKIP_FORMATS:
            logger.info(f"[#archive]è·³è¿‡æ ¼å¼åˆ—è¡¨ä¸ºç©ºï¼Œä¸è·³è¿‡ä»»ä½•æ–‡ä»¶: {archive_path}")
            return (archive_path, False, "")
            
        # 5. å¿«é€Ÿæ£€æŸ¥å‹ç¼©åŒ…å†…å®¹æ˜¯å¦åŒ…å«è·³è¿‡æ ¼å¼
        try:
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                # åªè·å–å‰10ä¸ªæ–‡ä»¶æ ·æœ¬ï¼Œé¿å…å¤„ç†å¤§å‹å‹ç¼©åŒ…
                sample_files = [f.filename for f in zip_ref.infolist()[:10]]
                
                # æ£€æŸ¥æ ·æœ¬æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«éœ€è¦è·³è¿‡çš„æ ¼å¼
                if any(f.lower().endswith(tuple(ACTIVE_SKIP_FORMATS)) for f in sample_files):
                    return (archive_path, True, "content")
        except (zipfile.BadZipFile, PermissionError, IOError) as e:
            logger.warning(f"[#archive]è¯»å–å‹ç¼©åŒ…æ—¶å‡ºé”™ (è·³è¿‡å†…å®¹æ£€æŸ¥): {archive_path}, Error: {e}")
            # å¦‚æœå‹ç¼©åŒ…æŸåæˆ–æ— æ³•è¯»å–ï¼Œä¸è·³è¿‡ï¼Œè®©åç»­éªŒè¯é˜¶æ®µå†³å®š
            return (archive_path, False, "")
            
        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œä¸éœ€è¦è·³è¿‡
        return (archive_path, False, "")
        
    except Exception as e:
        logger.error(f"[#archive]æ£€æŸ¥å‹ç¼©åŒ…è·³è¿‡çŠ¶æ€æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {archive_path}, Error: {e}")
        # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œä¸è·³è¿‡ï¼Œè®©åç»­éªŒè¯é˜¶æ®µå†³å®š
        return (archive_path, False, "")

def load_blacklist(file_path: Path) -> Set[str]:
    """åŠ è½½JSONæ ¼å¼çš„é»‘åå•æ–‡ä»¶"""
    if not file_path.exists():
        logger.info(f"[#file]é»‘åå•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return set()
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            blacklist_data = json.load(f)
            if isinstance(blacklist_data, list):
                # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„ä¸”è§„èŒƒåŒ–
                return {str(Path(p).resolve()) for p in blacklist_data}
            else:
                logger.warning(f"[#file]é»‘åå•æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºåˆ—è¡¨: {file_path}")
                return set()
    except json.JSONDecodeError:
        logger.error(f"[#file]è§£æé»‘åå•JSONæ–‡ä»¶å¤±è´¥: {file_path}")
        return set()
    except Exception as e:
        logger.error(f"[#file]åŠ è½½é»‘åå•æ–‡ä»¶æ—¶å‡ºé”™: {file_path}, é”™è¯¯: {e}")
        return set()


@infinite_monitor()
def monitor_and_process(paths: List[str], **kwargs) -> None:
    """ç›‘æ§æŒ‡å®šè·¯å¾„å¹¶å¤„ç†å‹ç¼©åŒ…
    
    Args:
        paths: éœ€è¦ç›‘æ§çš„è·¯å¾„åˆ—è¡¨
        **kwargs: å…¶ä»–å‚æ•°
    """
    # è·å–æ‰€æœ‰æ”¯æŒçš„å‹ç¼©åŒ…æ–‡ä»¶è·¯å¾„
    archive_paths = InputHandler.get_all_file_paths(set(paths), SUPPORTED_ARCHIVE_FORMATS)
    if not archive_paths:
        logger.info("[#file]æœªæ‰¾åˆ°æ”¯æŒçš„å‹ç¼©åŒ…æ–‡ä»¶")
        return

    # åŠ è½½JSONé»‘åå•
    json_blacklist = load_blacklist(BLACKLIST_FILE_PATH)
    if json_blacklist:
        logger.info(f"[#file]å·²åŠ è½½JSONé»‘åå•ï¼Œå…± {len(json_blacklist)} æ¡è®°å½•")

    # æ·»åŠ å¿«é€Ÿé¢„æ‰«æï¼Œæ£€æŸ¥å¹¶è¿‡æ»¤éœ€è¦è·³è¿‡çš„å‹ç¼©åŒ…
    logger.info(f"[#file]å¼€å§‹é¢„æ‰«æï¼Œå…±æ‰¾åˆ° {len(archive_paths)} ä¸ªå‹ç¼©åŒ…")
    logger.info(f"[#file]å½“å‰è·³è¿‡æ ¼å¼: {', '.join(ACTIVE_SKIP_FORMATS)}")
    logger.info(f"[#file]å½“å‰å‘½ä»¤è¡Œé»‘åå•è·¯å¾„å…³é”®è¯: {', '.join(ACTIVE_BLACKLIST_PATHS)}")
    # logger.info(f"[#file]è·³è¿‡å…³é”®è¯: {', '.join(SKIP_KEYWORDS)}")
    
    # è·å–çº¿ç¨‹æ•°
    thread_count, _, _ = get_performance_params()
    # thread_count = 16 # å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥å›ºå®šçº¿ç¨‹æ•°
    logger.info(f"[#performance]é¢„æ‰«æä½¿ç”¨ {thread_count} ä¸ªçº¿ç¨‹")
    
    # å¤šçº¿ç¨‹é¢„æ‰«æ
    filtered_archive_paths = []
    skipped_by_name = 0
    skipped_by_content = 0
    skipped_by_keyword_blacklist = 0
    skipped_by_json_blacklist = 0 # æ–°å¢è®¡æ•°å™¨
    
    # ä½¿ç”¨ partial å°† json_blacklist å›ºå®šä¸º check_archive_skip çš„å‚æ•°
    check_func = partial(check_archive_skip, json_blacklist=json_blacklist)
    
    with ThreadPoolExecutor(max_workers=thread_count) as executor:
        # å¹¶è¡Œæ‰§è¡Œæ£€æŸ¥
        results = list(executor.map(check_func, archive_paths))
        
        # å¤„ç†ç»“æœ
        for path, should_skip, skip_reason in results:
            if should_skip:
                if skip_reason == "name":
                    logger.info(f"[#archive]è·³è¿‡æ–‡ä»¶åæŒ‡ç¤ºå·²å¤„ç†çš„å‹ç¼©åŒ…: {path}")
                    skipped_by_name += 1
                elif skip_reason == "content":
                    logger.info(f"[#archive]è·³è¿‡å†…å®¹å«è·³è¿‡æ ¼å¼çš„å‹ç¼©åŒ…: {path}")
                    skipped_by_content += 1
                elif skip_reason == "keyword_blacklist": # ä¿®æ”¹åŸå› æ ‡è¯†
                    logger.info(f"[#archive]è·³è¿‡å‘½ä»¤è¡Œé»‘åå•è·¯å¾„çš„å‹ç¼©åŒ…: {path}")
                    skipped_by_keyword_blacklist += 1
                elif skip_reason == "json_blacklist": # æ–°å¢åŸå› å¤„ç†
                    logger.info(f"[#archive]è·³è¿‡JSONé»‘åå•ä¸­çš„å‹ç¼©åŒ…: {path}")
                    skipped_by_json_blacklist += 1
            else:
                filtered_archive_paths.append(path)
    
    # åœ¨å¤„ç†ç»“æœåå¯¹è¿‡æ»¤åçš„åˆ—è¡¨è¿›è¡Œæ’åº
    filtered_archive_paths.sort()
    logger.info(f"[#file]å·²å¯¹è¿‡æ»¤åçš„å‹ç¼©åŒ…è·¯å¾„è¿›è¡Œå‡åºæ’åº")

    skipped_total = skipped_by_name + skipped_by_content + skipped_by_keyword_blacklist + skipped_by_json_blacklist # æ›´æ–°æ€»æ•°
    logger.info(f"[#status]é¢„æ‰«æå®Œæˆï¼Œå…±è·³è¿‡ {skipped_total} ä¸ªå‹ç¼©åŒ…ï¼š")
    logger.info(f"[#status]- é€šè¿‡æ–‡ä»¶åè·³è¿‡ï¼š{skipped_by_name} ä¸ª")
    logger.info(f"[#status]- é€šè¿‡å†…å®¹æ£€æŸ¥è·³è¿‡ï¼š{skipped_by_content} ä¸ª")
    logger.info(f"[#status]- é€šè¿‡å‘½ä»¤è¡Œé»‘åå•è·¯å¾„è·³è¿‡ï¼š{skipped_by_keyword_blacklist} ä¸ª")
    logger.info(f"[#status]- é€šè¿‡JSONé»‘åå•æ–‡ä»¶è·³è¿‡ï¼š{skipped_by_json_blacklist} ä¸ª") # æ–°å¢æ—¥å¿—
    logger.info(f"[#status]å‰©ä½™å¾…å¤„ç†ï¼ˆå·²æ’åºï¼‰ï¼š{len(filtered_archive_paths)} ä¸ªå‹ç¼©åŒ…") # æ›´æ–°æ—¥å¿—è¯´æ˜å·²æ’åº
    
    # æ ¹æ®æ¨¡å¼å¤„ç†è¿‡æ»¤åçš„æ–‡ä»¶
    if filtered_archive_paths:
        # ç¡®ä¿å°† archive_path ä¼ é€’ç»™ process_archives
        process_archives(filtered_archive_paths, **kwargs)
    else:
        logger.info("[#status]æ²¡æœ‰éœ€è¦å¤„ç†çš„å‹ç¼©åŒ…æ–‡ä»¶")

def process_with_args(args):
    """å¤„ç†å‘½ä»¤è¡Œå‚æ•°çš„é€»è¾‘"""
    # å¤„ç†è·³è¿‡æ ¼å¼çš„å‚æ•°
    global ACTIVE_SKIP_FORMATS, ACTIVE_BLACKLIST_PATHS
    
    if args.skip is not None:
        # å¦‚æœæä¾›äº†è·³è¿‡æ ¼å¼å‚æ•°
        if args.skip.strip() == "":
            # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æ‰€æœ‰è·³è¿‡
            ACTIVE_SKIP_FORMATS = set()
            logger.info("[#file]å·²ç¦ç”¨æ‰€æœ‰è·³è¿‡æ ¼å¼")
        else:
            # å¦åˆ™ï¼Œè§£æé€—å·åˆ†éš”çš„æ ¼å¼åˆ—è¡¨
            formats = args.skip.strip().split(',')
            ACTIVE_SKIP_FORMATS = {fmt.strip() for fmt in formats if fmt.strip()}
            logger.info(f"[#file]å·²è®¾ç½®è‡ªå®šä¹‰è·³è¿‡æ ¼å¼: {', '.join(ACTIVE_SKIP_FORMATS)}")
    else:
        # ä½¿ç”¨é»˜è®¤è®¾ç½®
        ACTIVE_SKIP_FORMATS = SKIP_FORMATS.copy()
    
    # å¤„ç†é»‘åå•è·¯å¾„å…³é”®è¯çš„å‚æ•°
    if args.blacklist is not None:
        # å¦‚æœæä¾›äº†é»‘åå•è·¯å¾„å…³é”®è¯å‚æ•°
        if args.blacklist.strip() == "":
            # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æ‰€æœ‰é»‘åå•
            ACTIVE_BLACKLIST_PATHS = set()
            logger.info("[#file]å·²ç¦ç”¨æ‰€æœ‰é»‘åå•è·¯å¾„å…³é”®è¯")
        else:
            # å¦åˆ™ï¼Œè§£æé€—å·åˆ†éš”çš„å…³é”®è¯åˆ—è¡¨
            keywords = args.blacklist.strip().split(',')
            ACTIVE_BLACKLIST_PATHS = {kw.strip() for kw in keywords if kw.strip()}
            logger.info(f"[#file]å·²è®¾ç½®è‡ªå®šä¹‰é»‘åå•è·¯å¾„å…³é”®è¯: {', '.join(ACTIVE_BLACKLIST_PATHS)}")
    else:
        # ä½¿ç”¨é»˜è®¤è®¾ç½®
        ACTIVE_BLACKLIST_PATHS = BLACKLIST_PATHS.copy()
    
    # æ„å»ºè¿‡æ»¤å‚æ•°
    filter_params = {
        'min_width': args.min_width,
        'format': args.format,
        'quality': args.quality
    }

    # ä½¿ç”¨InputHandlerè·å–è¾“å…¥è·¯å¾„
    start_config_gui_thread()

    paths = InputHandler.get_input_paths(
        cli_paths=args.paths if args.paths else None,
        use_clipboard=args.clipboard,  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨å‰ªè´´æ¿
        allow_manual=True
    )
    if not paths:
        logger.info("[#file]æœªæä¾›æœ‰æ•ˆçš„å‹ç¼©åŒ…è·¯å¾„")
        return

    init_layout()
    
    # æ„å»ºå‚æ•°
    kwargs = {
        'filter_params': filter_params,
        'interval_minutes': args.interval if args.infinite else -1,
        'directories': paths,  # æ·»åŠ ç›®å½•åˆ—è¡¨
        'format': args.format,
        'quality': args.quality,
        'lossless': args.lossless,  # æ·»åŠ æ— æŸé€‰é¡¹
        **filter_params
    }
    
    # è°ƒç”¨ç›‘æ§å’Œå¤„ç†å‡½æ•°ï¼Œç”±è£…é¥°å™¨æ§åˆ¶æ˜¯å¦ä¸ºæ— é™æ¨¡å¼
    monitor_and_process(paths, **kwargs)

def main():
    
# å°† parser å®šä¹‰ä¿®æ”¹ä¸ºå¦‚ä¸‹
    parser = argparse.ArgumentParser(description='å‹ç¼©åŒ…å›¾ç‰‡å¤„ç†å·¥å…·')
    parser.add_argument('--min-width', type=int, default=0,
                    help='æœ€å°å›¾ç‰‡å®½åº¦(åƒç´ )ï¼Œä½äºæ­¤å®½åº¦çš„å‹ç¼©åŒ…å°†è¢«è·³è¿‡')
    parser.add_argument('--infinite', '-inf', action='store_true',
                    help='å¯ç”¨æ— é™å¾ªç¯ç›‘æ§æ¨¡å¼')
    parser.add_argument('--interval', type=int, default=10,
                    help='ç›‘æ§æ¨¡å¼çš„æ£€æŸ¥é—´éš”(åˆ†é’Ÿ)')
    parser.add_argument('--format', '-f', type=str, default='avif', 
                    choices=['avif', 'webp', 'jxl', 'jpg', 'png'],
                    help='ç›®æ ‡æ ¼å¼ (é»˜è®¤: avif)')
    parser.add_argument('--quality', '-q', type=int, default=90,
                    help='å‹ç¼©è´¨é‡ 1-100 (é»˜è®¤: 90)')
    parser.add_argument('--clipboard', '-c', action='store_true',
                    help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
    parser.add_argument('--lossless', '-l', action='store_true',
                    help='å¯ç”¨æ— æŸæ¨¡å¼')
    parser.add_argument('paths', nargs='*', help='å‹ç¼©åŒ…è·¯å¾„')
    parser.add_argument('--no-run', '-nr', action='store_true',
                    help='åªæ˜¾ç¤ºé…ç½®ç•Œé¢ï¼Œä¸æ‰§è¡Œè½¬æ¢')
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ç”¨äºè¦†ç›–è·³è¿‡æ ¼å¼
    parser.add_argument('--skip', type=str, 
                    help='è¦†ç›–è·³è¿‡æ ¼å¼ï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„åç¼€ååˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š.avif,.jxl,.webpï¼›è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²å¯ç¦ç”¨è·³è¿‡')
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ç”¨äºè¦†ç›–é»‘åå•è·¯å¾„å…³é”®è¯
    parser.add_argument('--blacklist', '-b', type=str, 
                    help='è¦†ç›–é»‘åå•è·¯å¾„å…³é”®è¯ï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„å…³é”®è¯åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼šbackup,temp,downloadsï¼›è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²å¯ç¦ç”¨é»‘åå•')
    parser.add_argument('--jxlfall', '-jf', action='store_true', 
                    help='å¯ç”¨JXLæ ¼å¼çš„é™çº§å¤„ç†')
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–TUIé…ç½®ç•Œé¢
    if len(sys.argv) > 1:
        args = parser.parse_args()
        process_with_args(args)
    else:
        # å®šä¹‰å¤é€‰æ¡†é€‰é¡¹

        # é¢„è®¾é…ç½® - ä» JSON åŠ è½½

        def on_run(params: dict):
            """TUIé…ç½®ç•Œé¢çš„å›è°ƒå‡½æ•°"""
            # å°†TUIå‚æ•°è½¬æ¢ä¸ºå‘½ä»¤è¡Œå‚æ•°æ ¼å¼
            sys.argv = [sys.argv[0]]
            
            # æ·»åŠ é€‰ä¸­çš„å¤é€‰æ¡†é€‰é¡¹
            for arg, enabled in params['options'].items():
                if enabled:
                    sys.argv.append(arg)
                    
            # æ·»åŠ è¾“å…¥æ¡†çš„å€¼
            for arg, value in params['inputs'].items():
                if value.strip():
                    sys.argv.append(arg)
                    sys.argv.append(value)
            
            # ä½¿ç”¨å…¨å±€çš„ parser è§£æå‚æ•°
            args = parser.parse_args()
            process_with_args(args)

        # åˆ›å»ºå¹¶è¿è¡Œé…ç½®ç•Œé¢
        # Check if --no-run flag is in the arguments
        no_run = "--no-run" in sys.argv or "-nr" in sys.argv
        app = create_config_app(
            program=__file__,
            parser=parser,
            title="PicsConvert",
            preset_configs=PRESET_CONFIGS, # ä½¿ç”¨ä» JSON åŠ è½½çš„ PRESET_CONFIGS
            on_run=False,
            rich_mode=USE_RICH,
            # if no_run else on_run,
        )
        if not USE_RICH:
            app.run()
        else:
            args = app.args  
            process_with_args(args)
if __name__ == '__main__':
    main()
